from dotenv import load_dotenv
from my_llm import MyLLM

load_dotenv()

llm = MyLLM(provider="modelscope")

messages = [{"role": "user", "content": "你好，请介绍一下你自己。"}]

response_stream = llm.think(messages)

for chunk in response_stream:
    print(chunk, end="", flush=True)